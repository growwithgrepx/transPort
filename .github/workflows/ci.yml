name: CI/CD Pipeline with Inline Reports

on:
  push:
    branches:
      - main
      - develop
      - fleet-enhancements
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  CACHE_VERSION: v2
  FLASK_ENV: testing
  TESTING: true

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    name: Unit & Integration Tests
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ matrix.python-version }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ matrix.python-version }}-
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-flask factory-boy coverage selenium webdriver-manager

      - name: Create test directories
        run: |
          mkdir -p test_results
          mkdir -p htmlcov

      - name: Check test directory structure
        run: |
          echo "Repository structure:"
          find . -type d -name "*test*" | head -20
          echo "Python files in tests:"
          find . -name "test_*.py" -o -name "*_test.py" | head -20

      - name: Run unit tests with coverage
        env:
          DATABASE_URL: sqlite:///tests/test_selenium.db
        run: |
          # Find and run unit tests with multiple discovery patterns
          if find . -name "test_*.py" -path "*/unit/*" | head -1 | grep -q .; then
            echo "Running unit tests from tests/unit/"
            pytest tests/unit/ \
              --cov=./ \
              --cov-report=xml:coverage.xml \
              --cov-report=html:htmlcov \
              --cov-report=term-missing \
              --junitxml=test_results/unit-test-results.xml \
              --html=test_results/unit-report.html \
              --self-contained-html \
              -v
          elif find . -name "test_*.py" -path "*/tests/*" | head -1 | grep -q .; then
            echo "Running tests from tests/ directory"
            pytest tests/ \
              --cov=./ \
              --cov-report=xml:coverage.xml \
              --cov-report=html:htmlcov \
              --cov-report=term-missing \
              --junitxml=test_results/unit-test-results.xml \
              --html=test_results/unit-report.html \
              --self-contained-html \
              -v
          elif find . -name "test_*.py" | head -1 | grep -q .; then
            echo "Running all discoverable tests"
            pytest . \
              --cov=./ \
              --cov-report=xml:coverage.xml \
              --cov-report=html:htmlcov \
              --cov-report=term-missing \
              --junitxml=test_results/unit-test-results.xml \
              --html=test_results/unit-report.html \
              --self-contained-html \
              -v
          else
            echo "No test files found, creating empty results"
            echo '<?xml version="1.0" encoding="UTF-8"?><testsuites tests="0" failures="0" errors="0" time="0"><testsuite name="empty" tests="0" failures="0" errors="0" time="0"></testsuite></testsuites>' > test_results/unit-test-results.xml
          fi

      # NEW: Generate coverage summary for display
      - name: Generate coverage summary
        if: always()
        run: |
          if [ -f "coverage.xml" ]; then
            echo "COVERAGE_SUMMARY<<EOF" >> $GITHUB_ENV
            coverage report --format=text >> $GITHUB_ENV
            echo "EOF" >> $GITHUB_ENV
          fi

      - name: Run integration tests (if separate directory exists)
        env:
          DATABASE_URL: sqlite:///tests/test_selenium.db
        run: |
          if [ -d "tests/integration" ] && find tests/integration -name "test_*.py" | head -1 | grep -q .; then
            echo "Running integration tests from tests/integration/"
            pytest tests/integration/ \
              --junitxml=test_results/integration-test-results.xml \
              --html=test_results/integration-report.html \
              --self-contained-html \
              -v
          else
            echo "No separate integration tests directory found"
          fi

      # NEW: Parse test results for inline display
      - name: Parse test results
        if: always()
        run: |
          # Parse JUnit XML for test summary
          if [ -f "test_results/unit-test-results.xml" ]; then
            python3 << 'EOF'
          import xml.etree.ElementTree as ET
          import os
          
          try:
              tree = ET.parse('test_results/unit-test-results.xml')
              root = tree.getroot()
              
              total_tests = int(root.get('tests', 0))
              failures = int(root.get('failures', 0))
              errors = int(root.get('errors', 0))
              passed = total_tests - failures - errors
              
              # Write to environment for later use
              with open(os.environ['GITHUB_ENV'], 'a') as f:
                  f.write(f'TESTS_TOTAL={total_tests}\n')
                  f.write(f'TESTS_PASSED={passed}\n')
                  f.write(f'TESTS_FAILED={failures}\n')
                  f.write(f'TESTS_ERROR={errors}\n')
                  
              # Get individual test results
              test_results = []
              for testcase in root.findall('.//testcase'):
                  classname = testcase.get('classname', '')
                  name = testcase.get('name', '')
                  time = testcase.get('time', '0')
                  
                  failure = testcase.find('failure')
                  error = testcase.find('error')
                  
                  if failure is not None:
                      status = "❌ FAILED"
                      message = failure.get('message', '')[:100] + "..." if len(failure.get('message', '')) > 100 else failure.get('message', '')
                  elif error is not None:
                      status = "🔥 ERROR"
                      message = error.get('message', '')[:100] + "..." if len(error.get('message', '')) > 100 else error.get('message', '')
                  else:
                      status = "✅ PASSED"
                      message = ""
                  
                  test_results.append(f"| {classname}.{name} | {status} | {time}s | {message} |")
              
              # Write test results table
              with open(os.environ['GITHUB_ENV'], 'a') as f:
                  f.write('TEST_RESULTS_TABLE<<EOF\n')
                  f.write('| Test | Status | Time | Message |\n')
                  f.write('|------|--------|------|----------|\n')
                  for result in test_results[:20]:  # Limit to first 20 tests
                      f.write(result + '\n')
                  if len(test_results) > 20:
                      f.write(f'| ... and {len(test_results) - 20} more tests | | | |\n')
                  f.write('EOF\n')
                  
          except Exception as e:
              print(f"Error parsing test results: {e}")
          EOF
          fi

      # NEW: Enhanced step summary with inline results
      - name: Enhanced test results summary
        if: always()
        run: |
          echo "## ✅ Unit & Integration Test Summary - Python ${{ matrix.python-version }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Test results summary
          if [ -n "$TESTS_TOTAL" ]; then
            echo "### 📊 Test Results Overview" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Tests**: $TESTS_TOTAL" >> $GITHUB_STEP_SUMMARY
            echo "- **Passed**: $TESTS_PASSED ✅" >> $GITHUB_STEP_SUMMARY
            echo "- **Failed**: $TESTS_FAILED ❌" >> $GITHUB_STEP_SUMMARY
            echo "- **Errors**: $TESTS_ERROR 🔥" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Calculate pass rate
            if [ "$TESTS_TOTAL" -gt 0 ]; then
              PASS_RATE=$((TESTS_PASSED * 100 / TESTS_TOTAL))
              echo "- **Pass Rate**: $PASS_RATE%" >> $GITHUB_STEP_SUMMARY
            fi
          fi
          
          # Coverage summary
          if [ -n "$COVERAGE_SUMMARY" ]; then
            echo "### 📈 Coverage Summary" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "$COVERAGE_SUMMARY" >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi
          
          # Individual test results
          if [ -n "$TEST_RESULTS_TABLE" ]; then
            echo "### 🔍 Individual Test Results" >> $GITHUB_STEP_SUMMARY
            echo "$TEST_RESULTS_TABLE" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
          fi

      # NEW: Upload coverage to GitHub Pages (if enabled)
      - name: Deploy coverage to GitHub Pages
        if: github.ref == 'refs/heads/main' && matrix.python-version == '3.11'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./htmlcov
          destination_dir: coverage

      # NEW: Coverage comment on PR
      - name: Coverage comment
        if: github.event_name == 'pull_request' && matrix.python-version == '3.11'
        uses: py-cov-action/python-coverage-comment-action@v3
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          COVERAGE_FILE: coverage.xml

      - name: Publish Test Results
        uses: dorny/test-reporter@v1
        if: always() && hashFiles('test_results/*.xml') != ''
        with:
          name: Unit & Integration Tests (${{ matrix.python-version }})
          path: test_results/*.xml
          reporter: java-junit
          fail-on-error: false

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test_results/
            coverage.xml
            htmlcov/
          retention-days: 30
          if-no-files-found: warn

  selenium-tests:
    runs-on: ubuntu-latest
    name: Selenium UI Tests
    needs: unit-tests
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-selenium-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-selenium-
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-flask factory-boy coverage selenium webdriver-manager

      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Start Xvfb (for headless browser)
        run: |
          sudo apt-get install -y xvfb
          Xvfb :99 & export DISPLAY=:99

      - name: Create test directories
        run: mkdir -p test_results

      - name: Run Selenium tests
        env:
          DISPLAY: :99
          DATABASE_URL: sqlite:///tests/test_selenium.db
          CHROME_HEADLESS: true
        run: |
          # Multiple discovery patterns for selenium tests
          if [ -d "tests/selenium_tests" ] && find tests/selenium_tests -name "test_*.py" | head -1 | grep -q .; then
            echo "Running selenium tests from tests/selenium_tests/"
            pytest tests/selenium_tests/ \
              --junitxml=test_results/selenium-test-results.xml \
              --html=test_results/selenium-report.html \
              --self-contained-html \
              -v
          elif [ -d "tests/selenium" ] && find tests/selenium -name "test_*.py" | head -1 | grep -q .; then
            echo "Running selenium tests from tests/selenium/"
            pytest tests/selenium/ \
              --junitxml=test_results/selenium-test-results.xml \
              --html=test_results/selenium-report.html \
              --self-contained-html \
              -v
          elif find . -name "*selenium*test*.py" | head -1 | grep -q .; then
            echo "Running selenium tests by filename pattern"
            pytest $(find . -name "*selenium*test*.py") \
              --junitxml=test_results/selenium-test-results.xml \
              --html=test_results/selenium-report.html \
              --self-contained-html \
              -v
          else
            echo "No selenium tests found, creating empty results"
            echo '<?xml version="1.0" encoding="UTF-8"?><testsuites tests="0" failures="0" errors="0" time="0"><testsuite name="empty" tests="0" failures="0" errors="0" time="0"></testsuite></testsuites>' > test_results/selenium-test-results.xml
          fi

      # NEW: Parse Selenium test results
      - name: Parse Selenium test results
        if: always()
        run: |
          if [ -f "test_results/selenium-test-results.xml" ]; then
            python3 << 'EOF'
          import xml.etree.ElementTree as ET
          import os
          
          try:
              tree = ET.parse('test_results/selenium-test-results.xml')
              root = tree.getroot()
              
              total_tests = int(root.get('tests', 0))
              failures = int(root.get('failures', 0))
              errors = int(root.get('errors', 0))
              passed = total_tests - failures - errors
              
              with open(os.environ['GITHUB_ENV'], 'a') as f:
                  f.write(f'SELENIUM_TESTS_TOTAL={total_tests}\n')
                  f.write(f'SELENIUM_TESTS_PASSED={passed}\n')
                  f.write(f'SELENIUM_TESTS_FAILED={failures}\n')
                  f.write(f'SELENIUM_TESTS_ERROR={errors}\n')
          except Exception as e:
              print(f"Error parsing selenium test results: {e}")
          EOF
          fi

      # NEW: Enhanced Selenium test summary
      - name: Enhanced Selenium test summary
        if: always()
        run: |
          echo "## 🌐 Selenium Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -n "$SELENIUM_TESTS_TOTAL" ]; then
            echo "### 📊 UI Test Results Overview" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Tests**: $SELENIUM_TESTS_TOTAL" >> $GITHUB_STEP_SUMMARY
            echo "- **Passed**: $SELENIUM_TESTS_PASSED ✅" >> $GITHUB_STEP_SUMMARY
            echo "- **Failed**: $SELENIUM_TESTS_FAILED ❌" >> $GITHUB_STEP_SUMMARY
            echo "- **Errors**: $SELENIUM_TESTS_ERROR 🔥" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "$SELENIUM_TESTS_TOTAL" -gt 0 ]; then
              PASS_RATE=$((SELENIUM_TESTS_PASSED * 100 / SELENIUM_TESTS_TOTAL))
              echo "- **Pass Rate**: $PASS_RATE%" >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: Publish Test Results
        uses: dorny/test-reporter@v1
        if: always() && hashFiles('test_results/*.xml') != ''
        with:
          name: Selenium UI Tests
          path: test_results/*.xml
          reporter: java-junit
          fail-on-error: false

      - name: Upload Selenium test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: selenium-test-results
          path: |
            test_results/
            tests/test_screenshots/
          retention-days: 30
          if-no-files-found: warn

  # NEW: Generate comprehensive report
  generate-report:
    runs-on: ubuntu-latest
    name: Generate Comprehensive Report
    needs: [unit-tests, selenium-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Generate comprehensive report
        run: |
          echo "# 📊 Comprehensive Test Report - Run #${{ github.run_number }}" > report.md
          echo "" >> report.md
          echo "Generated on: $(date)" >> report.md
          echo "" >> report.md
          
          # Summary section
          echo "## 📈 Summary" >> report.md
          echo "" >> report.md
          
          # Process each Python version
          for version in 3.9 3.10 3.11; do
            if [ -f "artifacts/test-results-$version/test_results/unit-test-results.xml" ]; then
              echo "### Python $version Results" >> report.md
              python3 << EOF
          import xml.etree.ElementTree as ET
          
          try:
              tree = ET.parse('artifacts/test-results-$version/test_results/unit-test-results.xml')
              root = tree.getroot()
              
              total = int(root.get('tests', 0))
              failures = int(root.get('failures', 0))
              errors = int(root.get('errors', 0))
              passed = total - failures - errors
              
              print(f"- **Total Tests**: {total}")
              print(f"- **Passed**: {passed} ✅")
              print(f"- **Failed**: {failures} ❌")
              print(f"- **Errors**: {errors} 🔥")
              if total > 0:
                  print(f"- **Pass Rate**: {passed * 100 // total}%")
              print("")
          except Exception as e:
              print(f"Error processing Python $version results: {e}")
          EOF
            fi
          done

      - name: Comment PR with comprehensive report
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const report = fs.readFileSync('report.md', 'utf8');
              
              // Add links to coverage report and artifacts
              const fullReport = report + `
          
              ## 🔗 Quick Links
              - 📊 [Detailed Test Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
              - 📈 [Coverage Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
              - 📁 [Download Artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
              `;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: fullReport
              });
            } catch (error) {
              console.error('Error posting comprehensive report:', error);
            }
