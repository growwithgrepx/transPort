name: CI/CD Pipeline with Inline Reports

on:
  push:
    branches:
      - main
      - fleet-enhancements
  pull_request:
    branches:
      - main
      - fleet-enhancements

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11']
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      - name: Run unit tests
        run: |
          mkdir -p test_results tests/reports/htmlcov/unit
          pytest --junitxml=test_results/unit-test-results.xml --cov=tests --cov-report=xml:tests/reports/coverage_unit.xml --cov-report=html:tests/reports/htmlcov/unit
      - name: Debug artifact paths
        run: |
          echo "Contents of test_results:"
          ls -la test_results/
          echo "Contents of tests/reports:"
          ls -la tests/reports/
      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test_results/
            tests/reports/coverage_unit.xml
            tests/reports/htmlcov/unit/
          retention-days: 30
          if-no-files-found: warn

  selenium-tests:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
      - name: Run Selenium tests
        run: |
          mkdir -p test_results tests/test_screenshots
          pytest tests/selenium_tests/ --junitxml=test_results/selenium-test-results.xml
      - name: Debug artifact paths
        run: |
          echo "Contents of test_results:"
          ls -la test_results/
          echo "Contents of tests/test_screenshots:"
          ls -la tests/test_screenshots/
      - name: Upload Selenium test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: selenium-test-results
          path: |
            test_results/
            tests/test_screenshots/
          retention-days: 30
          if-no-files-found: warn

  generate-report:
    runs-on: ubuntu-latest
    name: Generate Comprehensive Report
    needs: [unit-tests, selenium-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
      - name: Debug artifact structure
        run: |
          echo "=== Artifact Structure ==="
          find artifacts -type f -name "*.xml" -print0 | xargs -0 -I {} ls -la {} | head -20 || echo "No XML files found"
          echo "=== Directory Structure ==="
          ls -la artifacts/ || echo "Artifacts directory is empty or does not exist"
          echo "=== Test Results Files ==="
          find artifacts -name "*test-results.xml" -exec ls -la {} \; || echo "No test results files found"
      - name: Generate comprehensive report
        run: |
          export PYTHONIOENCODING=utf-8
          export LC_ALL=en_US.UTF-8
          echo "# üìä Comprehensive Test Report - Run #${{ github.run_number }}" > report.md
          echo "" >> report.md
          echo "Generated on: $(date)" >> report.md
          echo "" >> report.md

          # Process unit tests
          xml_file="artifacts/test-results-3.11/test_results/unit-test-results.xml"
          cov_file="artifacts/test-results-3.11/tests/reports/coverage_unit.xml"
          if [ -f "$xml_file" ]; then
            echo "### üêç Python 3.11 Unit & Integration Tests" >> report.md
            python3 scripts/generate_unit_report.py --xml "$xml_file" --cov "$cov_file" --out /tmp/unit_3.11.txt >> report.md
          else
            echo "‚ö†Ô∏è No unit test results found for Python 3.11" >> report.md
            echo "" >> report.md
          fi

          # Process Selenium tests
          selenium_xml="artifacts/selenium-test-results/test_results/selenium-test-results.xml"
          if [ -f "$selenium_xml" ]; then
            echo "### üåê Selenium UI Tests" >> report.md
            python3 scripts/generate_selenium_report.py --xml "$selenium_xml" --out /tmp/selenium.txt >> report.md
          else
            echo "‚ö†Ô∏è No Selenium test results found" >> report.md
            echo "" >> report.md
          fi

          # Summary Table
          echo "### üìã Summary Table" >> report.md
          echo "| Category | Total Tests | Passed ‚úÖ | Failed ‚ùå | Errors üî• | Pass Rate | Coverage |" >> report.md
          echo "|----------|-------------|-----------|-----------|-----------|-----------|----------|" >> report.md
          if [ -f /tmp/unit_3.11.txt ]; then
            cat /tmp/unit_3.11.txt
            IFS=',' read total passed failed errors coverage < /tmp/unit_3.11.txt
            passrate=0
            if [ "$total" -gt 0 ]; then passrate=$((passed * 100 / total)); fi
            echo "| Unit & Integration | $total | $passed | $failed | $errors | ${passrate}% | ${coverage}% |" >> report.md
          fi
          if [ -f /tmp/selenium.txt ]; then
            cat /tmp/selenium.txt
            IFS=',' read total passed failed errors < /tmp/selenium.txt
            passrate=0
            if [ "$total" -gt 0 ]; then passrate=$((passed * 100 / total)); fi
            echo "| UI Tests | $total | $passed | $failed | $errors | ${passrate}% | N/A |" >> report.md
          fi
          if [ -f /tmp/unit_3.11.txt ] && [ -f /tmp/selenium.txt ]; then
            IFS=',' read utotal upassed ufailed uerrors ucoverage < /tmp/unit_3.11.txt
            IFS=',' read stotal spassed sfailed serrors < /tmp/selenium.txt
            gtotal=$((utotal + stotal))
            gpassed=$((upassed + spassed))
            gfailed=$((ufailed + sfailed))
            gerrors=$((uerrors + serrors))
            gpassrate=0
            if [ "$gtotal" -gt 0 ]; then gpassrate=$((gpassed * 100 / gtotal)); fi
            echo "| **Total** | **$gtotal** | **$gpassed** | **$gfailed** | **$gerrors** | **${gpassrate}%** | N/A |" >> report.md
          fi
          echo "" >> report.md
          echo "## üéØ Overall Summary" >> report.md
          echo "" >> report.md
          echo "### üîó Detailed Reports" >> report.md
          echo "- [Unit & Integration Test Results (Python 3.11)](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)" >> report.md
          echo "- [Selenium UI Test Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)" >> report.md
          echo "- [Coverage Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}/artifacts)" >> report.md

      - name: Debug /tmp summary files
        run: |
          echo "/tmp/unit_3.11.txt contents:"
          cat /tmp/unit_3.11.txt || echo "unit_3.11.txt not found"
          echo "/tmp/selenium.txt contents:"
          cat /tmp/selenium.txt || echo "selenium.txt not found"

      - name: Display report content
        run: |
          echo "=== Generated Report Content ==="
          cat report.md

      - name: Generate step summary (Pipeline Summary First)
        run: |
          echo "## üìä Pipeline Summary" > $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat report.md >> $GITHUB_STEP_SUMMARY

      - name: Comment PR with comprehensive report
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            try {
              const report = fs.readFileSync('report.md', 'utf8');
              const repoUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}`;
              const runUrl = `${repoUrl}/actions/runs/${context.runId}`;
              const branch = context.ref.replace('refs/heads/', '');
              const fullReport = report + [
                '',
                '## üîó Quick Links',
                `- üìä [View Full Pipeline Results](${runUrl})`,
                `- üìà [Coverage Report](https://${context.repo.owner}.github.io/${context.repo.repo}/coverage/${branch}/index.html)`,
                `- üìÅ [Download All Artifacts](${runUrl}/artifacts)`,
                `- üè† [Repository](${repoUrl})`,
                '',
                '---',
                '*This report was automatically generated by the CI/CD pipeline*'
              ].join('\n');
              await github.rest.issues.createComment({
                ...context.issue,
                body: fullReport
              });
            } catch (error) {
              console.error('Error posting comprehensive report:', error);
              console.error('Error details:', error.message);
            }

      - name: Copy HTML coverage report to artifacts for direct linking
        run: |
          mkdir -p artifacts/coverage_html
          cp -r artifacts/test-results-3.11/tests/reports/htmlcov/unit/* artifacts/coverage_html/ || true
