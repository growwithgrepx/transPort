name: CI/CD Pipeline with Inline Reports

on:
  push:
    branches:
      - main
      - develop
      - fleet-enhancements
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  CACHE_VERSION: v2
  FLASK_ENV: testing
  TESTING: true

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    name: Unit & Integration Tests
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.11']  # Updated to run only on Python 3.11
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ matrix.python-version }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ matrix.python-version }}-
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-flask factory-boy coverage selenium webdriver-manager

      - name: Create test directories
        run: |
          mkdir -p test_results
          mkdir -p htmlcov

      - name: Run unit tests with coverage
        env:
          DATABASE_URL: sqlite:///tests/test_selenium.db
        run: |
          echo "Discovered test files:"
          find . -name "test_*.py" -o -name "*_test.py"
          pytest tests/ --verbose --cov=./ --cov-report=xml:coverage.xml --cov-report=html:htmlcov --junitxml=test_results/unit-test-results.xml

      - name: Deploy coverage to GitHub Pages
        if: github.ref == 'refs/heads/main' && matrix.python-version == '3.11'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./htmlcov
          destination_dir: coverage

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test_results/
            coverage.xml
            htmlcov/
          retention-days: 30
          if-no-files-found: warn
  
  selenium-tests:
    runs-on: ubuntu-latest
    name: Selenium UI Tests
    needs: unit-tests
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-selenium-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-selenium-
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-flask factory-boy coverage selenium webdriver-manager

      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Start Xvfb (for headless browser)
        run: |
          sudo apt-get install -y xvfb
          Xvfb :99 & export DISPLAY=:99

      - name: Create test directories
        run: mkdir -p test_results

      - name: Run Selenium tests
        env:
          DISPLAY: :99
          DATABASE_URL: sqlite:///tests/test_selenium.db
          CHROME_HEADLESS: true
        run: |
          # Multiple discovery patterns for selenium tests
          if [ -d "tests/selenium_tests" ] && find tests/selenium_tests -name "test_*.py" | head -1 | grep -q .; then
            echo "Running selenium tests from tests/selenium_tests/"
            pytest tests/selenium_tests/ \
              --junitxml=test_results/selenium-test-results.xml \
              --html=test_results/selenium-report.html \
              --self-contained-html \
              -v
          elif [ -d "tests/selenium" ] && find tests/selenium -name "test_*.py" | head -1 | grep -q .; then
            echo "Running selenium tests from tests/selenium/"
            pytest tests/selenium/ \
              --junitxml=test_results/selenium-test-results.xml \
              --html=test_results/selenium-report.html \
              --self-contained-html \
              -v
          elif find . -name "*selenium*test*.py" | head -1 | grep -q .; then
            echo "Running selenium tests by filename pattern"
            pytest $(find . -name "*selenium*test*.py") \
              --junitxml=test_results/selenium-test-results.xml \
              --html=test_results/selenium-report.html \
              --self-contained-html \
              -v
          else
            echo "No selenium tests found, creating empty results"
            echo '<?xml version="1.0" encoding="UTF-8"?><testsuites tests="0" failures="0" errors="0" time="0"><testsuite name="empty" tests="0" failures="0" errors="0" time="0"></testsuite></testsuites>' > test_results/selenium-test-results.xml
          fi

      # NEW: Parse Selenium test results
      - name: Parse Selenium test results
        if: always()
        run: |
          if [ -f "test_results/selenium-test-results.xml" ]; then
            python3 << 'EOF'
          import xml.etree.ElementTree as ET
          import os
          
          try:
              tree = ET.parse('test_results/selenium-test-results.xml')
              root = tree.getroot()
              
              total_tests = int(root.get('tests', 0))
              failures = int(root.get('failures', 0))
              errors = int(root.get('errors', 0))
              passed = total_tests - failures - errors
              
              with open(os.environ['GITHUB_ENV'], 'a') as f:
                  f.write(f'SELENIUM_TESTS_TOTAL={total_tests}\n')
                  f.write(f'SELENIUM_TESTS_PASSED={passed}\n')
                  f.write(f'SELENIUM_TESTS_FAILED={failures}\n')
                  f.write(f'SELENIUM_TESTS_ERROR={errors}\n')
          except Exception as e:
              print(f"Error parsing selenium test results: {e}")
          EOF
          fi

      # NEW: Enhanced Selenium test summary
      - name: Enhanced Selenium test summary
        if: always()
        run: |
          echo "## üåê Selenium Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -n "$SELENIUM_TESTS_TOTAL" ]; then
            echo "### üìä UI Test Results Overview" >> $GITHUB_STEP_SUMMARY
            echo "- **Total Tests**: $SELENIUM_TESTS_TOTAL" >> $GITHUB_STEP_SUMMARY
            echo "- **Passed**: $SELENIUM_TESTS_PASSED ‚úÖ" >> $GITHUB_STEP_SUMMARY
            echo "- **Failed**: $SELENIUM_TESTS_FAILED ‚ùå" >> $GITHUB_STEP_SUMMARY
            echo "- **Errors**: $SELENIUM_TESTS_ERROR üî•" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            if [ "$SELENIUM_TESTS_TOTAL" -gt 0 ]; then
              PASS_RATE=$((SELENIUM_TESTS_PASSED * 100 / SELENIUM_TESTS_TOTAL))
              echo "- **Pass Rate**: $PASS_RATE%" >> $GITHUB_STEP_SUMMARY
            fi
          fi

      - name: Publish Test Results
        uses: dorny/test-reporter@v1
        if: always() && hashFiles('test_results/*.xml') != ''
        with:
          name: Selenium UI Tests
          path: test_results/*.xml
          reporter: java-junit
          fail-on-error: false

      - name: Upload Selenium test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: selenium-test-results
          path: |
            test_results/
            tests/test_screenshots/
          retention-days: 30
          if-no-files-found: warn

  # NEW: Generate comprehensive report
  generate-report:
    runs-on: ubuntu-latest
    name: Generate Comprehensive Report
    needs: [unit-tests, selenium-tests]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts

      - name: Debug artifact structure
        run: |
          echo "=== Artifact Structure ==="
          find artifacts -type f -name "*.xml" | head -20
          echo "=== Directory Structure ==="
          ls -la artifacts/
          echo "=== Test Results Files ==="
          find artifacts -name "*test-results.xml" -exec ls -la {} \;

      - name: Generate comprehensive report
        run: |
          echo "# üìä Comprehensive Test Report - Run #${{ github.run_number }}" > report.md
          echo "" >> report.md
          echo "Generated on: $(date)" >> report.md
          echo "" >> report.md
          
          # Summary section
          echo "## üìà Test Results Summary" >> report.md
          echo "" >> report.md
          
          # Initialize counters
          total_unit_tests=0
          total_unit_passed=0
          total_unit_failed=0
          total_unit_errors=0
          total_unit_coverage=0
          
          total_selenium_tests=0
          total_selenium_passed=0
          total_selenium_failed=0
          total_selenium_errors=0
          
          # Process each Python version for unit tests
          for version in 3.9 3.10 3.11; do
            xml_file="artifacts/test-results-$version/test_results/unit-test-results.xml"
            coverage_file="artifacts/test-results-$version/coverage.xml"
            if [ -f "$xml_file" ]; then
              echo "### üêç Python $version Unit Tests" >> report.md
              python3 << EOF >> report.md
          import xml.etree.ElementTree as ET
          import sys
          
          try:
              tree = ET.parse('$xml_file')
              root = tree.getroot()
              
              if root.tag == 'testsuites':
                  total = sum(int(suite.get('tests', 0)) for suite in root.findall('testsuite'))
                  failures = sum(int(suite.get('failures', 0)) for suite in root.findall('testsuite'))
                  errors = sum(int(suite.get('errors', 0)) for suite in root.findall('testsuite'))
              else:
                  total = int(root.get('tests', 0))
                  failures = int(root.get('failures', 0))
                  errors = int(root.get('errors', 0))
              
              passed = total - failures - errors
              
              print(f"- **Total Tests**: {total}")
              print(f"- **Passed**: {passed} ‚úÖ")
              print(f"- **Failed**: {failures} ‚ùå")
              print(f"- **Errors**: {errors} üî•")
              if total > 0:
                  print(f"- **Pass Rate**: {passed * 100 // total}%")
              print("")
              
              # Extract coverage if available
              coverage = 0
              if os.path.exists('$coverage_file'):
                  cov_tree = ET.parse('$coverage_file')
                  cov_root = cov_tree.getroot()
                  coverage = float(cov_root.get('line-rate', 0)) * 100
                  print(f"- **Code Coverage**: {coverage:.2f}%")
              
              with open(f'/tmp/unit_$version.txt', 'w') as f:
                  f.write(f"{total},{passed},{failures},{errors},{coverage}")
                  
          except Exception as e:
              print(f"‚ùå Error processing Python $version results: {e}")
              print("")
          EOF
            else
              echo "‚ö†Ô∏è No test results found for Python $version" >> report.md
              echo "" >> report.md
            fi
          done
          
          # Process Selenium tests
          selenium_xml="artifacts/selenium-test-results/test_results/selenium-test-results.xml"
          if [ -f "$selenium_xml" ]; then
            echo "### üåê Selenium UI Tests" >> report.md
            python3 << EOF >> report.md
          import xml.etree.ElementTree as ET
          
          try:
              tree = ET.parse('$selenium_xml')
              root = tree.getroot()
              
              if root.tag == 'testsuites':
                  total = sum(int(suite.get('tests', 0)) for suite in root.findall('testsuite'))
                  failures = sum(int(suite.get('failures', 0)) for suite in root.findall('testsuite'))
                  errors = sum(int(suite.get('errors', 0)) for suite in root.findall('testsuite'))
              else:
                  total = int(root.get('tests', 0))
                  failures = int(root.get('failures', 0))
                  errors = int(root.get('errors', 0))
              
              passed = total - failures - errors
              
              print(f"- **Total Tests**: {total}")
              print(f"- **Passed**: {passed} ‚úÖ")
              print(f"- **Failed**: {failures} ‚ùå")
              print(f"- **Errors**: {errors} üî•")
              if total > 0:
                  print(f"- **Pass Rate**: {passed * 100 // total}%")
              print("")
              
              with open('/tmp/selenium.txt', 'w') as f:
                  f.write(f"{total},{passed},{failures},{errors}")
                  
          except Exception as e:
              print(f"‚ùå Error processing Selenium results: {e}")
              print("")
          EOF
          else
            echo "‚ö†Ô∏è No Selenium test results found" >> report.md
            echo "" >> report.md
          fi
          
          # Generate overall summary with table
          echo "## üéØ Overall Summary" >> report.md
          echo "" >> report.md
          
          python3 << EOF >> report.md
          import os
          
          # Aggregate unit test results
          unit_total, unit_passed, unit_failed, unit_errors, unit_coverage = 0, 0, 0, 0, 0
          for version in ['3.9', '3.10', '3.11']:
              try:
                  with open(f'/tmp/unit_{version}.txt', 'r') as f:
                      data = f.read().strip().split(',')
                      unit_total += int(data[0])
                      unit_passed += int(data[1])
                      unit_failed += int(data[2])
                      unit_errors += int(data[3])
                      unit_coverage += float(data[4])
              except:
                  pass
          
          # Average coverage across Python versions
          if unit_total > 0:
              unit_coverage = unit_coverage / 3  # Since we have 3 Python versions
          
          # Aggregate selenium results
          selenium_total, selenium_passed, selenium_failed, selenium_errors = 0, 0, 0, 0
          try:
              with open('/tmp/selenium.txt', 'r') as f:
                  data = f.read().strip().split(',')
                  selenium_total = int(data[0])
                  selenium_passed = int(data[1])
                  selenium_failed = int(data[2])
                  selenium_errors = int(data[3])
          except:
              pass
          
          # Overall totals
          grand_total = unit_total + selenium_total
          grand_passed = unit_passed + selenium_passed
          grand_failed = unit_failed + selenium_failed
          grand_errors = unit_errors + selenium_errors
          
          # Summary Table
          print(f"### üìã Summary Table")
          print(f"| Category | Total Tests | Passed ‚úÖ | Failed ‚ùå | Errors üî• | Pass Rate | Coverage |")
          print(f"|----------|-------------|-----------|-----------|-----------|-----------|----------|")
          print(f"| Unit Tests | {unit_total} | {unit_passed} | {unit_failed} | {unit_errors} | {unit_passed * 100 // unit_total if unit_total > 0 else 0}% | {unit_coverage:.2f}% |")
          print(f"| UI Tests | {selenium_total} | {selenium_passed} | {selenium_failed} | {selenium_errors} | {selenium_passed * 100 // selenium_total if selenium_total > 0 else 0}% | N/A |")
          print(f"| **Total** | **{grand_total}** | **{grand_passed}** | **{grand_failed}** | **{grand_errors}** | **{grand_passed * 100 // grand_total if grand_total > 0 else 0}%** | N/A |")
          print("")
          
          # Links to detailed reports
          print(f"### üîó Detailed Reports")
          print(f"- [Unit Test Results (Python 3.9)](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})")
          print(f"- [Unit Test Results (Python 3.10)](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})")
          print(f"- [Unit Test Results (Python 3.11)](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})")
          print(f"- [Selenium UI Test Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})")
          print(f"- [Coverage Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})")
          print("")
          EOF

      - name: Display report content
        run: |
          echo "=== Generated Report Content ==="
          cat report.md

      - name: Generate step summary
        run: |
          echo "## üìä Pipeline Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          cat report.md >> $GITHUB_STEP_SUMMARY

      - name: Comment PR with comprehensive report
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const report = fs.readFileSync('report.md', 'utf8');
              
              // Generate proper URLs
              const repoUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}`;
              const runUrl = `${repoUrl}/actions/runs/${context.runId}`;
              
              // Add links to coverage report and artifacts
              const fullReport = report + `

            ## üîó Quick Links
            - üìä [View Full Pipeline Results](${runUrl})
            - üìà [Coverage Report](${repoUrl}/actions/runs/${context.runId})
            - üìÅ [Download All Artifacts](${runUrl})
            - üè† [Repository](${repoUrl})
            
            ---
            *This report was automatically generated by the CI/CD pipeline*
            `;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: fullReport
              });
            } catch (error) {
              console.error('Error posting comprehensive report:', error);
              console.error('Error details:', error.message);
            }
