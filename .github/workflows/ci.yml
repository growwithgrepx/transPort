name: CI/CD Pipeline

on:
  push:
    branches:
      - main
      - develop
      - fleet-enhancements
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.11'
  CACHE_VERSION: v2
  FLASK_ENV: testing
  TESTING: true

jobs:
  unit-tests:
    runs-on: ubuntu-latest
    name: Unit & Integration Tests
    strategy:
      fail-fast: false  # Don't cancel other jobs if one fails
      matrix:
        python-version: ['3.9', '3.10', '3.11']  # Fixed: 3.1 -> 3.10
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ matrix.python-version }}-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-${{ matrix.python-version }}-
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-flask factory-boy coverage selenium webdriver-manager

      - name: Create test directories
        run: |
          mkdir -p test_results
          mkdir -p htmlcov

      - name: Check test directory structure
        run: |
          echo "Repository structure:"
          find . -type d -name "*test*" | head -20
          echo "Python files in tests:"
          find . -name "test_*.py" -o -name "*_test.py" | head -20

      - name: Run unit tests with coverage
        env:
          DATABASE_URL: sqlite:///tests/test_selenium.db
        run: |
          # Find and run unit tests with multiple discovery patterns
          if find . -name "test_*.py" -path "*/unit/*" | head -1 | grep -q .; then
            echo "Running unit tests from tests/unit/"
            pytest tests/unit/ \
              --cov=./ \
              --cov-report=xml:coverage.xml \
              --cov-report=html:htmlcov \
              --junitxml=test_results/unit-test-results.xml \
              --html=test_results/unit-report.html \
              --self-contained-html \
              -v
          elif find . -name "test_*.py" -path "*/tests/*" | head -1 | grep -q .; then
            echo "Running tests from tests/ directory"
            pytest tests/ \
              --cov=./ \
              --cov-report=xml:coverage.xml \
              --cov-report=html:htmlcov \
              --junitxml=test_results/unit-test-results.xml \
              --html=test_results/unit-report.html \
              --self-contained-html \
              -v
          elif find . -name "test_*.py" | head -1 | grep -q .; then
            echo "Running all discoverable tests"
            pytest . \
              --cov=./ \
              --cov-report=xml:coverage.xml \
              --cov-report=html:htmlcov \
              --junitxml=test_results/unit-test-results.xml \
              --html=test_results/unit-report.html \
              --self-contained-html \
              -v
          else
            echo "No test files found, creating empty results"
            echo '<?xml version="1.0" encoding="UTF-8"?><testsuites tests="0" failures="0" errors="0" time="0"><testsuite name="empty" tests="0" failures="0" errors="0" time="0"></testsuite></testsuites>' > test_results/unit-test-results.xml
          fi

      - name: Run integration tests (if separate directory exists)
        env:
          DATABASE_URL: sqlite:///tests/test_selenium.db
        run: |
          if [ -d "tests/integration" ] && find tests/integration -name "test_*.py" | head -1 | grep -q .; then
            echo "Running integration tests from tests/integration/"
            pytest tests/integration/ \
              --junitxml=test_results/integration-test-results.xml \
              --html=test_results/integration-report.html \
              --self-contained-html \
              -v
          else
            echo "No separate integration tests directory found"
          fi

      - name: Verify test results were created
        run: |
          echo "Contents of test_results directory:"
          ls -la test_results/ || echo "No test_results directory"
          echo "Contents of root directory (looking for coverage.xml):"
          ls -la *.xml || echo "No XML files in root"
          echo "Contents of htmlcov directory:"
          ls -la htmlcov/ || echo "No htmlcov directory"

      - name: Display test results summary
        if: always()
        run: |
          echo "## ✅ Unit & Integration Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "🐍 **Python Version**: ${{ matrix.python-version }}" >> $GITHUB_STEP_SUMMARY
          
          if [ -f "test_results/unit-test-results.xml" ]; then
            echo "📊 **Unit Test Results**: Available in artifacts" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Unit Test Results**: No results found" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f "test_results/integration-test-results.xml" ]; then
            echo "🔗 **Integration Test Results**: Available in artifacts" >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f "coverage.xml" ]; then
            echo "📈 **Coverage Report**: Available in artifacts" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Publish Test Results
        uses: dorny/test-reporter@v1
        if: always() && hashFiles('test_results/*.xml') != ''
        with:
          name: Unit & Integration Tests (${{ matrix.python-version }})
          path: test_results/*.xml
          reporter: java-junit
          fail-on-error: false

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            test_results/
            coverage.xml
            htmlcov/
          retention-days: 30
          if-no-files-found: warn

  selenium-tests:
    runs-on: ubuntu-latest
    name: Selenium UI Tests
    needs: unit-tests
    if: always()  # Run even if unit tests fail
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-selenium-${{ hashFiles('requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-selenium-
            ${{ runner.os }}-pip-${{ env.CACHE_VERSION }}-
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov pytest-flask factory-boy coverage selenium webdriver-manager

      - name: Install Chrome
        run: |
          wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
          sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list'
          sudo apt-get update
          sudo apt-get install -y google-chrome-stable

      - name: Start Xvfb (for headless browser)
        run: |
          sudo apt-get install -y xvfb
          Xvfb :99 & export DISPLAY=:99

      - name: Create test directories
        run: mkdir -p test_results

      - name: Check for Selenium tests
        run: |
          echo "Looking for Selenium test files:"
          find . -name "*selenium*" -type f | head -10
          find . -name "*ui*test*" -type f | head -10
          find . -name "test_*" -path "*selenium*" | head -10

      - name: Run Selenium tests
        env:
          DISPLAY: :99
          DATABASE_URL: sqlite:///tests/test_selenium.db
          CHROME_HEADLESS: true
        run: |
          # Multiple discovery patterns for selenium tests
          if [ -d "tests/selenium_tests" ] && find tests/selenium_tests -name "test_*.py" | head -1 | grep -q .; then
            echo "Running selenium tests from tests/selenium_tests/"
            pytest tests/selenium_tests/ \
              --junitxml=test_results/selenium-test-results.xml \
              --html=test_results/selenium-report.html \
              --self-contained-html \
              -v
          elif [ -d "tests/selenium" ] && find tests/selenium -name "test_*.py" | head -1 | grep -q .; then
            echo "Running selenium tests from tests/selenium/"
            pytest tests/selenium/ \
              --junitxml=test_results/selenium-test-results.xml \
              --html=test_results/selenium-report.html \
              --self-contained-html \
              -v
          elif find . -name "*selenium*test*.py" | head -1 | grep -q .; then
            echo "Running selenium tests by filename pattern"
            pytest $(find . -name "*selenium*test*.py") \
              --junitxml=test_results/selenium-test-results.xml \
              --html=test_results/selenium-report.html \
              --self-contained-html \
              -v
          else
            echo "No selenium tests found, creating empty results"
            echo '<?xml version="1.0" encoding="UTF-8"?><testsuites tests="0" failures="0" errors="0" time="0"><testsuite name="empty" tests="0" failures="0" errors="0" time="0"></testsuite></testsuites>' > test_results/selenium-test-results.xml
          fi

      - name: Display test results summary
        if: always()
        run: |
          echo "## 🌐 Selenium Test Summary" >> $GITHUB_STEP_SUMMARY
          if [ -f "test_results/selenium-test-results.xml" ]; then
            echo "📊 **Selenium Results**: Available in artifacts" >> $GITHUB_STEP_SUMMARY
          else
            echo "⚠️ **Selenium Results**: No results found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Publish Test Results
        uses: dorny/test-reporter@v1
        if: always() && hashFiles('test_results/*.xml') != ''
        with:
          name: Selenium UI Tests
          path: test_results/*.xml
          reporter: java-junit
          fail-on-error: false

      - name: Upload Selenium test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: selenium-test-results
          path: |
            test_results/
            tests/test_screenshots/
          retention-days: 30
          if-no-files-found: warn

      - name: Comment PR with test results
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            try {
              let comment = `## 🧪 CI/CD Pipeline Results - Run #${{ github.run_number }}\n\n`;
              
              // Unit test results
              const unitXmlPath = path.join('test_results', 'unit-test-results.xml');
              if (fs.existsSync(unitXmlPath)) {
                const unitXml = fs.readFileSync(unitXmlPath, 'utf8');
                const unitTotal = parseInt(unitXml.match(/tests="(\d+)"/)?.[1] || '0', 10);
                const unitFailures = parseInt(unitXml.match(/failures="(\d+)"/)?.[1] || '0', 10);
                const unitErrors = parseInt(unitXml.match(/errors="(\d+)"/)?.[1] || '0', 10);
                const unitPassed = unitTotal - unitFailures - unitErrors;
                
                comment += `### 📊 Unit & Integration Tests\n`;
                comment += `**Summary**: ${unitPassed}/${unitTotal} tests passed `;
                comment += (unitFailures > 0 || unitErrors > 0)
                  ? `❌ (${unitFailures} failures, ${unitErrors} errors)\n\n`
                  : `✅\n\n`;
              }
              
              // Selenium test results
              const seleniumXmlPath = path.join('test_results', 'selenium-test-results.xml');
              if (fs.existsSync(seleniumXmlPath)) {
                const seleniumXml = fs.readFileSync(seleniumXmlPath, 'utf8');
                const seleniumTotal = parseInt(seleniumXml.match(/tests="(\d+)"/)?.[1] || '0', 10);
                const seleniumFailures = parseInt(seleniumXml.match(/failures="(\d+)"/)?.[1] || '0', 10);
                const seleniumErrors = parseInt(seleniumXml.match(/errors="(\d+)"/)?.[1] || '0', 10);
                const seleniumPassed = seleniumTotal - seleniumFailures - seleniumErrors;
                
                comment += `### 🌐 Selenium UI Tests\n`;
                comment += `**Summary**: ${seleniumPassed}/${seleniumTotal} tests passed `;
                comment += (seleniumFailures > 0 || seleniumErrors > 0)
                  ? `❌ (${seleniumFailures} failures, ${seleniumErrors} errors)\n\n`
                  : `✅\n\n`;
              }
              
              comment += `📊 **View Details**: [Pipeline Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})\n`;
              comment += `📁 **Artifacts**: Test reports and coverage available in workflow artifacts\n`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.error('Error posting comment:', error);
            }
